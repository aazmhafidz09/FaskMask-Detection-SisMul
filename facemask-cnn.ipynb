{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import random\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "import imutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image  as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'facemask-dataset/yes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-9b85e58bcc67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The number of images with facemask labelled 'yes':\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'facemask-dataset/yes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The number of images with facemask labelled 'no':\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'facemask-dataset/no'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'facemask-dataset/yes'"
     ]
    }
   ],
   "source": [
    "print(\"The number of images with facemask labelled 'yes':\",len(os.listdir('facemask-dataset/yes')))\n",
    "print(\"The number of images with facemask labelled 'no':\",len(os.listdir('facemask-dataset/no')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 1372\n",
      "Percentage of positive examples: 50.0%, number of pos examples: 686\n",
      "Percentage of negative examples: 50.0%, number of neg examples: 686\n"
     ]
    }
   ],
   "source": [
    "def data_summary(main_path):\n",
    "    \n",
    "    yes_path = main_path+'yesreal'\n",
    "    no_path = main_path+'noreal'\n",
    "        \n",
    "    # number of files (images) that are in the the folder named 'yes' that represent tumorous (positive) examples\n",
    "    m_pos = len(listdir(yes_path))\n",
    "    # number of files (images) that are in the the folder named 'no' that represent non-tumorous (negative) examples\n",
    "    m_neg = len(listdir(no_path))\n",
    "    # number of all examples\n",
    "    m = (m_pos+m_neg)\n",
    "    \n",
    "    pos_prec = (m_pos* 100.0)/ m\n",
    "    neg_prec = (m_neg* 100.0)/ m\n",
    "    \n",
    "    print(f\"Number of examples: {m}\")\n",
    "    print(f\"Percentage of positive examples: {pos_prec}%, number of pos examples: {m_pos}\") \n",
    "    print(f\"Percentage of negative examples: {neg_prec}%, number of neg examples: {m_neg}\") \n",
    "    \n",
    "augmented_data_path = 'dataset/facemask-dataset/trial1/augmented data1/'    \n",
    "data_summary(augmented_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
    "    dataset = []\n",
    "    \n",
    "    for unitData in os.listdir(SOURCE):\n",
    "        data = SOURCE + unitData\n",
    "        if(os.path.getsize(data) > 0):\n",
    "            dataset.append(unitData)\n",
    "        else:\n",
    "            print('Skipped ' + unitData)\n",
    "            print('Invalid file i.e zero size')\n",
    "    \n",
    "    train_set_length = int(len(dataset) * SPLIT_SIZE)\n",
    "    test_set_length = int(len(dataset) - train_set_length)\n",
    "    shuffled_set = random.sample(dataset, len(dataset))\n",
    "    train_set = dataset[0:train_set_length]\n",
    "    test_set = dataset[-test_set_length:]\n",
    "       \n",
    "    for unitData in train_set:\n",
    "        temp_train_set = SOURCE + unitData\n",
    "        final_train_set = TRAINING + unitData\n",
    "        copyfile(temp_train_set, final_train_set)\n",
    "    \n",
    "    for unitData in test_set:\n",
    "        temp_test_set = SOURCE + unitData\n",
    "        final_test_set = TESTING + unitData\n",
    "        copyfile(temp_test_set, final_test_set)\n",
    "        \n",
    "        \n",
    "YES_SOURCE_DIR = \"C:\\\\Users\\ASUS\\Karima/facemask-dataset/trial1/augmented data1/yesreal/\"\n",
    "TRAINING_YES_DIR = \"C:\\\\Users\\ASUS\\Karima/facemask-dataset/trial1/augmented data1/training/yes1/\"\n",
    "TESTING_YES_DIR = \"C:\\\\Users\\ASUS\\Karima/facemask-dataset/trial1/augmented data1/testing/yes1/\"\n",
    "NO_SOURCE_DIR = \"C:\\\\Users\\ASUS\\Karima/facemask-dataset/trial1/augmented data1/noreal/\"\n",
    "TRAINING_NO_DIR = \"C:\\\\Users\\ASUS\\Karima/facemask-dataset/trial1/augmented data1/training/no1/\"\n",
    "TESTING_NO_DIR = \"C:\\\\Users\\ASUS\\Karima/facemask-dataset/trial1/augmented data1/testing/no1/\"\n",
    "split_size = .8\n",
    "split_data(YES_SOURCE_DIR, TRAINING_YES_DIR, TESTING_YES_DIR, split_size)\n",
    "split_data(NO_SOURCE_DIR, TRAINING_NO_DIR, TESTING_NO_DIR, split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of images with facemask in the training set labelled 'yes': 548\n",
      "The number of images with facemask in the test set labelled 'yes': 138\n",
      "The number of images without facemask in the training set labelled 'no': 548\n",
      "The number of images without facemask in the test set labelled 'no': 138\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of images with facemask in the training set labelled 'yes':\", len(os.listdir('dataset/facemask-dataset/trial1/augmented data1/training/yes1')))\n",
    "print(\"The number of images with facemask in the test set labelled 'yes':\", len(os.listdir('dataset/facemask-dataset/trial1/augmented data1/testing/yes1')))\n",
    "print(\"The number of images without facemask in the training set labelled 'no':\", len(os.listdir('dataset/facemask-dataset/trial1/augmented data1/training/no1')))\n",
    "print(\"The number of images without facemask in the test set labelled 'no':\", len(os.listdir('dataset/facemask-dataset/trial1/augmented data1/testing/no1')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(100, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(100, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1096 images belonging to 2 classes.\n",
      "Found 276 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = \"dataset/facemask-dataset/trial1/augmented data1/training\"\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, \n",
    "                                                    batch_size=10, \n",
    "                                                    target_size=(150, 150))\n",
    "VALIDATION_DIR = \"dataset/facemask-dataset/trial1/augmented data1/testing\"\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, \n",
    "                                                         batch_size=10, \n",
    "                                                         target_size=(150, 150))\n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "110/110 [==============================] - 106s 955ms/step - loss: 0.1741 - acc: 0.9297 - val_loss: 0.1024 - val_acc: 0.9674\n",
      "Epoch 2/30\n",
      "110/110 [==============================] - 105s 936ms/step - loss: 0.1517 - acc: 0.9480 - val_loss: 0.1071 - val_acc: 0.9819\n",
      "Epoch 3/30\n",
      "110/110 [==============================] - 283s 3s/step - loss: 0.1243 - acc: 0.9526 - val_loss: 0.1319 - val_acc: 0.9601\n",
      "Epoch 4/30\n",
      "110/110 [==============================] - 258s 2s/step - loss: 0.1425 - acc: 0.9489 - val_loss: 0.0371 - val_acc: 0.9855\n",
      "INFO:tensorflow:Assets written to: model-004.model\\assets\n",
      "Epoch 5/30\n",
      "110/110 [==============================] - 96s 856ms/step - loss: 0.0973 - acc: 0.9690 - val_loss: 0.0547 - val_acc: 0.9746\n",
      "Epoch 6/30\n",
      "110/110 [==============================] - 100s 899ms/step - loss: 0.0959 - acc: 0.9672 - val_loss: 0.0626 - val_acc: 0.9891\n",
      "Epoch 7/30\n",
      "110/110 [==============================] - 109s 958ms/step - loss: 0.1109 - acc: 0.9644 - val_loss: 0.0777 - val_acc: 0.9674\n",
      "Epoch 8/30\n",
      "110/110 [==============================] - 97s 871ms/step - loss: 0.0790 - acc: 0.9790 - val_loss: 0.0420 - val_acc: 0.9855\n",
      "Epoch 9/30\n",
      "110/110 [==============================] - 91s 798ms/step - loss: 0.1045 - acc: 0.9653 - val_loss: 0.1543 - val_acc: 0.9638\n",
      "Epoch 10/30\n",
      "110/110 [==============================] - 91s 813ms/step - loss: 0.1213 - acc: 0.9608 - val_loss: 0.0385 - val_acc: 0.9928\n",
      "Epoch 11/30\n",
      "110/110 [==============================] - 95s 845ms/step - loss: 0.2000 - acc: 0.9443 - val_loss: 0.1489 - val_acc: 0.9783\n",
      "Epoch 12/30\n",
      "110/110 [==============================] - 144s 1s/step - loss: 0.1140 - acc: 0.9599 - val_loss: 0.0460 - val_acc: 0.9783\n",
      "Epoch 13/30\n",
      "110/110 [==============================] - 109s 987ms/step - loss: 0.0860 - acc: 0.9708 - val_loss: 0.1144 - val_acc: 0.9710\n",
      "Epoch 14/30\n",
      "110/110 [==============================] - 101s 912ms/step - loss: 0.0902 - acc: 0.9699 - val_loss: 0.0526 - val_acc: 0.9783\n",
      "Epoch 15/30\n",
      "110/110 [==============================] - 107s 959ms/step - loss: 0.2816 - acc: 0.8841 - val_loss: 0.2271 - val_acc: 0.9348\n",
      "Epoch 16/30\n",
      "110/110 [==============================] - 103s 927ms/step - loss: 0.1982 - acc: 0.9279 - val_loss: 0.0625 - val_acc: 0.9819\n",
      "Epoch 17/30\n",
      "110/110 [==============================] - 90s 807ms/step - loss: 0.1256 - acc: 0.9553 - val_loss: 0.1042 - val_acc: 0.9710\n",
      "Epoch 18/30\n",
      "110/110 [==============================] - 96s 861ms/step - loss: 0.1193 - acc: 0.9571 - val_loss: 0.0401 - val_acc: 0.9928\n",
      "Epoch 19/30\n",
      "110/110 [==============================] - 91s 822ms/step - loss: 0.0897 - acc: 0.9699 - val_loss: 0.0236 - val_acc: 0.9928\n",
      "INFO:tensorflow:Assets written to: model-019.model\\assets\n",
      "Epoch 20/30\n",
      "110/110 [==============================] - 96s 862ms/step - loss: 0.0460 - acc: 0.9845 - val_loss: 0.0206 - val_acc: 1.0000\n",
      "INFO:tensorflow:Assets written to: model-020.model\\assets\n",
      "Epoch 21/30\n",
      "110/110 [==============================] - 93s 834ms/step - loss: 0.1051 - acc: 0.9626 - val_loss: 0.0291 - val_acc: 0.9891\n",
      "Epoch 22/30\n",
      "110/110 [==============================] - 96s 857ms/step - loss: 0.0812 - acc: 0.9754 - val_loss: 0.0286 - val_acc: 0.9928\n",
      "Epoch 23/30\n",
      "110/110 [==============================] - 98s 878ms/step - loss: 0.0978 - acc: 0.9626 - val_loss: 0.0609 - val_acc: 0.9746\n",
      "Epoch 24/30\n",
      "110/110 [==============================] - 94s 838ms/step - loss: 0.0606 - acc: 0.9808 - val_loss: 0.0196 - val_acc: 1.0000\n",
      "INFO:tensorflow:Assets written to: model-024.model\\assets\n",
      "Epoch 25/30\n",
      "110/110 [==============================] - 99s 889ms/step - loss: 0.0846 - acc: 0.9681 - val_loss: 0.0720 - val_acc: 0.9674\n",
      "Epoch 26/30\n",
      "110/110 [==============================] - 95s 845ms/step - loss: 0.0554 - acc: 0.9818 - val_loss: 0.0146 - val_acc: 1.0000\n",
      "INFO:tensorflow:Assets written to: model-026.model\\assets\n",
      "Epoch 27/30\n",
      "110/110 [==============================] - 98s 877ms/step - loss: 0.0436 - acc: 0.9881 - val_loss: 0.1148 - val_acc: 0.9638\n",
      "Epoch 28/30\n",
      "110/110 [==============================] - 98s 873ms/step - loss: 0.0442 - acc: 0.9827 - val_loss: 0.0252 - val_acc: 0.9855\n",
      "Epoch 29/30\n",
      "110/110 [==============================] - 99s 883ms/step - loss: 0.0734 - acc: 0.9754 - val_loss: 0.0299 - val_acc: 0.9891\n",
      "Epoch 30/30\n",
      "110/110 [==============================] - 100s 900ms/step - loss: 0.0795 - acc: 0.9781 - val_loss: 0.0146 - val_acc: 0.9928\n",
      "INFO:tensorflow:Assets written to: model-030.model\\assets\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=30,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_clsfr=cv2.CascadeClassifier('dataset/facemask-dataset/haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict={0:'with_mask',1:'without_mask'}\n",
    "color_dict={0:(0,255,0),1:(0,0,255)}\n",
    "\n",
    "size = 4\n",
    "webcam = cv2.VideoCapture(0) #Use camera 0\n",
    "\n",
    "# We load the xml file\n",
    "classifier = cv2.CascadeClassifier('dataset/facemask-dataset/haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    (rval, im) = webcam.read()\n",
    "    im=cv2.flip(im,1,1) #Flip to act as a mirror\n",
    "\n",
    "    # Resize the image to speed up detection\n",
    "    mini = cv2.resize(im, (im.shape[1] // size, im.shape[0] // size))\n",
    "\n",
    "    # detect MultiScale / faces \n",
    "    faces = classifier.detectMultiScale(mini)\n",
    "\n",
    "    # Draw rectangles around each face\n",
    "    for f in faces:\n",
    "        (x, y, w, h) = [v * size for v in f] #Scale the shapesize backup\n",
    "        #Save just the rectangle faces in SubRecFaces\n",
    "        face_img = im[y:y+h, x:x+w]\n",
    "        resized=cv2.resize(face_img,(150,150))\n",
    "        normalized=resized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,150,150,3))\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        result=model.predict(reshaped)\n",
    "        #print(result)\n",
    "        \n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "      \n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(im,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(im, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "    # Show the image\n",
    "    cv2.imshow('LIVE',   im)\n",
    "    key = cv2.waitKey(10)\n",
    "    # if Esc key is press then break out of the loop \n",
    "    if key == 27: #The Esc key\n",
    "        break\n",
    "# Stop video\n",
    "webcam.release()\n",
    "\n",
    "# Close all started windows\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
